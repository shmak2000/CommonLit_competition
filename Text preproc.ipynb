{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cb8604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import torch\n",
    "import sentence_transformers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, pipeline, BartModel\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoConfig, RobertaConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4b81c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_name = 'roberta-large'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53665768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_model():\n",
    "    configuration = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    configuration.hidden_dropout_prob = 0.0\n",
    "    configuration.attention_probs_dropout_prob = 0.0\n",
    "    configuration.classifier_dropout = 0.0\n",
    "\n",
    "    return configuration\n",
    "\n",
    "config = config_model()\n",
    "size = config.hidden_size\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b5588",
   "metadata": {},
   "source": [
    "# Objects and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d4bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_path = r\"dataset\\summaries_train.csv\"\n",
    "summaries_test_path = r\"dataset\\summaries_test.csv\"\n",
    "prompts_train_path = r\"dataset\\prompts_train.csv\"\n",
    "prompts_test_path = r\"dataset\\prompts_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ffb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normilizing score to values 0 to 1\"\"\"\n",
    "    df -= np.min(df)\n",
    "    df /= df.max()\n",
    "    print('Normilized' if df.min() == 0.0 and df.max() == 1.0 else 'NormError:wrong values')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def short_text(text, max_length=400) -> str:\n",
    "    if len(text.split()) > 700:\n",
    "        middle_point = text[len(text)//2:].index('.') + len(text)//2 + 1\n",
    "        text_pt1 = text[:middle_point]\n",
    "        text_pt2 = text[middle_point:]\n",
    "        \n",
    "        text = short_text(text_pt1) + short_text(text_pt2)\n",
    "        \n",
    "    summarized = summarizer(text, max_length=max_length, min_length=200, do_sample=False)\n",
    "    \n",
    "    return summarized[0]['summary_text']\n",
    "\n",
    "def moving_average(array: np.array, betta=0.9) -> np.array:\n",
    "    \"\"\"\n",
    "    Computing moving average with bias correction.\n",
    "    \"\"\"\n",
    "    \n",
    "    V = 0\n",
    "    average_array = np.zeros(len(array))\n",
    "    for i in range(len(array)):\n",
    "        V = betta * V + (1 - betta) * array[i]\n",
    "        average_array[i] = V/(1 - pow(betta, i+1))\n",
    "        \n",
    "    return average_array\n",
    "\n",
    "\n",
    "\n",
    "def mean_pooling(outputs, batch) -> torch.tensor:\n",
    "    \n",
    "    attention_mask = batch['attention_mask']\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    \n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    \n",
    "    mean_pooled = summed/summed_mask\n",
    "    \n",
    "    return mean_pooled\n",
    "\n",
    "def normilize(df_column: pd.DataFrame):\n",
    "    \n",
    "    df_column = (df_column - df_column.mean())/df_column.std()\n",
    "    \n",
    "    return df_column\n",
    "\n",
    "def get_stat_features(df, text_col=\"text\"):\n",
    "    \n",
    "    df[\"num_unique_words\"] = normilize(df[text_col].apply(lambda x: len(set(x.split()))))\n",
    "    df[\"num_words\"] = normilize(df[text_col].apply(lambda x: len(x.split())))\n",
    "    df[\"num_sentences\"] = normilize(df[text_col].apply(lambda x: len(x.split('.'))))\n",
    "    \n",
    "    df[\"syntax_count\"] = normilize(df[text_col].apply(lambda x: x.count(\",\") \n",
    "                                                      + x.count(\"-\") + x.count(\";\") + x.count(\":\")))\n",
    "    df['smog_index'] = normilize(df[text_col].apply(lambda x: textstat.smog_index(x)))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e88d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data\n",
    "\n",
    "class SentenseData(Dataset):\n",
    "    \"\"\"\n",
    "    :params: path to csv file with summaries, path to csv file with prompts, 'score' param\\\n",
    "    defines which score is used: content/wording\n",
    "    \"\"\"\n",
    "    def __init__(self, summaries_path=summaries_train_path,\n",
    "                 prompts_path=prompts_train_path,\n",
    "                 score='content'):\n",
    "        \n",
    "        with open(summaries_path, encoding='utf-8') as f:\n",
    "            summaries = pd.read_csv(f)\n",
    "            summaries = get_stat_features(summaries)\n",
    "            \n",
    "            self.summaries = summaries\n",
    "            \n",
    "        with open(prompts_path, encoding='utf-8') as f:\n",
    "            self.prompts = pd.read_csv(f)\n",
    "        \n",
    "        self.score_type = score\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__summaries)\n",
    "    \n",
    "    @property\n",
    "    def summaries(self):\n",
    "        return self.__summaries\n",
    "    \n",
    "    @summaries.setter\n",
    "    def summaries(self, df):\n",
    "        # df = df[df.prompt_id != '3b9047'].reset_index(drop=True)\n",
    "        self.__summaries = df\n",
    "\n",
    "    @property\n",
    "    def prompts(self):\n",
    "        return self.__prompts\n",
    "    \n",
    "    @prompts.setter\n",
    "    def prompts(self, file):\n",
    "        # file = file[file.prompt_id != '3b9047'].reset_index(drop=True)\n",
    "        self.__prompts = file\n",
    "    \n",
    "    def get_batch_text(self, index):\n",
    "        summary_text = self.summaries.text[index]\n",
    "\n",
    "        res = tokenizer(summary_text, padding='max_length', return_tensors='pt', truncation=True)\n",
    "        res = {k:val.squeeze() for k, val in res.items()}\n",
    "        res['features'] = torch.tensor([self.summaries['num_unique_words'][index], self.summaries['num_words'][index],\n",
    "                                  self.summaries['num_sentences'][index],self.summaries['syntax_count'][index],\n",
    "                                  self.summaries['smog_index'][index]], dtype=torch.float32)\n",
    "            \n",
    "        return res\n",
    "        \n",
    "    def get_score(self, index):\n",
    "        if self.score_type == 'content':\n",
    "            score = self.summaries.content[index]\n",
    "            \n",
    "        elif self.score_type == 'wording':\n",
    "            score = self.summaries.wording[index]\n",
    "        \n",
    "        return torch.tensor(score)\n",
    "        \n",
    "    def __getitem__(self, index) -> torch.tensor:\n",
    "\n",
    "        batch_text = self.get_batch_text(index)\n",
    "        batch_score = self.get_score(index)\n",
    "        \n",
    "        return batch_text, batch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a6970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentenseData(Dataset):\n",
    "#     \"\"\"\n",
    "#     :params: path to csv file with summaries, path to csv file with prompts, 'score' param\\\n",
    "#     defines which score is used: content/wording\n",
    "#     \"\"\"\n",
    "#     def __init__(self, summaries_path=summaries_train_path,\n",
    "#                  prompts_path=prompts_train_path,\n",
    "#                  score='content', test=False):\n",
    "#         self.test = test\n",
    "        \n",
    "#         with open(summaries_path, encoding='utf-8') as f:\n",
    "#             self.summaries = pd.read_csv(f)\n",
    "            \n",
    "#         with open(prompts_path, encoding='utf-8') as f:\n",
    "#             self.prompts = pd.read_csv(f)\n",
    "        \n",
    "# #         self.summaries.content = norm_score(self.summaries.content)\n",
    "# #         self.summaries.wording = norm_score(self.summaries.wording)\n",
    "        \n",
    "#         self.score_type = score\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.__summaries)\n",
    "    \n",
    "#     @property\n",
    "#     def summaries(self):\n",
    "#         return self.__summaries\n",
    "    \n",
    "#     @summaries.setter\n",
    "#     def summaries(self, df):\n",
    "#         if self.test:\n",
    "#             self.__summaries = df[df.prompt_id == '39c16e'].reset_index(drop=True)\n",
    "            \n",
    "#         else:\n",
    "#             self.__summaries = df[df.prompt_id != '39c16e'].reset_index(drop=True)\n",
    "    \n",
    "#     @property\n",
    "#     def prompts(self):\n",
    "#         return self.__prompts\n",
    "    \n",
    "#     @prompts.setter\n",
    "#     def prompts(self, file):\n",
    "#         self.__prompts = file\n",
    "# #         for i, text in enumerate(self.__prompts.prompt_text):\n",
    "# #             if len(text.split()) > 600:\n",
    "# #                 self.__prompts.prompt_text[i] = short_text(text)\n",
    "    \n",
    "#     def get_batch_text(self, index):\n",
    "#         summary_text = self.summaries.text[index]\n",
    "        \n",
    "#         if True:#self.score_type == 'wording':\n",
    "#             res = tokenizer(summary_text, padding='max_length', return_tensors='pt', truncation=True)\n",
    "#             return {k:val.squeeze() for k, val in res.items()}\n",
    "        \n",
    "#         prompt_text = self.prompts.prompt_text[self.prompts.prompt_id ==\n",
    "#                                                self.summaries.prompt_id[index]].item().replace('\\n','')\n",
    "        \n",
    "#         return tokenizer([summary_text, prompt_text], padding='max_length', return_tensors='pt', truncation=True)\n",
    "    \n",
    "#     def get_score(self, index):\n",
    "#         if self.score_type == 'content':\n",
    "#             score = self.summaries.content[index]\n",
    "            \n",
    "#         elif self.score_type == 'wording':\n",
    "#             score = self.summaries.wording[index]\n",
    "        \n",
    "#         return torch.tensor(score)\n",
    "        \n",
    "#     def __getitem__(self, index) -> torch.tensor:\n",
    "\n",
    "#         batch_text = self.get_batch_text(index)\n",
    "#         batch_score = self.get_score(index)\n",
    "        \n",
    "#         return batch_text, batch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac301561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CosineSimilarityLoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CosineSimilarityLoss, self).__init__()\n",
    "#         self.loss_func = nn.MSELoss()\n",
    "#         self.identity = nn.Identity()\n",
    "#         self.cos_sim = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         embedding_1 = torch.stack([sentence[0] for sentence in x])\n",
    "#         embedding_2 = torch.stack([sentence[1] for sentence in x])\n",
    "        \n",
    "#         cos_score = self.cos_sim(embedding_1, embedding_2)\n",
    "#         cos_score = self.identity(cos_score)\n",
    "        \n",
    "#         loss = self.loss_func(x, y)\n",
    "        \n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b49a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Реализовать батч лоадер, возвращающий с каждой итерацией батч - лист с bs количеством словарей\n",
    "# # Нужно, чтобы умел с шафлом\n",
    "#\n",
    "# class BatchLoader(DataLoader):\n",
    "#     def __init__(self, dataset, batch_size=1, shuffle=False, drop=False):\n",
    "# #         super(BatchLoader, self).__init__()\n",
    "#\n",
    "#         self.dataset = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         self.shuffle = shuffle\n",
    "#         self.drop = drop\n",
    "#         self.dataset_len = len(dataset)\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return math.ceil(self.dataset_len/self.batch_size)\n",
    "#\n",
    "#     def __iter__(self):\n",
    "#         self.index = list(range(self.dataset_len))\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(self.index)\n",
    "#         return self\n",
    "#\n",
    "#     def __next__(self):\n",
    "#         batch_index = []\n",
    "#\n",
    "#         if self.batch_size >= len(self.index):\n",
    "#             if self.drop or not self.index:\n",
    "#                 raise StopIteration\n",
    "#             else:\n",
    "#                 batch_index = self.index.copy()\n",
    "#                 self.index.clear()\n",
    "#         else:\n",
    "#             for _ in range(self.batch_size):\n",
    "#                 batch_index.append(self.index.pop())\n",
    "#\n",
    "#         batch = []\n",
    "#         target = []\n",
    "#\n",
    "#         for i in batch_index:\n",
    "#             batch.append(self.dataset[i][0])\n",
    "#             target.append(self.dataset[i][1])\n",
    "#\n",
    "#         return batch, torch.tensor(target)\n",
    "#\n",
    "#     def __getitem__(self, index):\n",
    "#         batch = self.dataset[index][0]\n",
    "#         target = self.dataset[index][1]\n",
    "#\n",
    "#         return batch, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf60f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence Semantic Similarity Bert model\n",
    "    :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "    :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "    \"\"\"\n",
    "    def __init__(self, with_features=False, input_size=size):\n",
    "        super(STSBertModel, self).__init__()\n",
    "\n",
    "        self.word_embedding = RobertaModel.from_pretrained(model_name, config=config)\n",
    "        self.cos_score = nn.CosineSimilarity(dim=0)\n",
    "        self.identity = nn.Identity()\n",
    "        self.input_size = input_size\n",
    "        self.with_features = with_features\n",
    "\n",
    "        if with_features:\n",
    "            self.n_features = 5\n",
    "            self.input_size += self.n_features\n",
    "\n",
    "        # for param in self.word_embedding.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.attention = SelfAttention(self.input_size)\n",
    "        self.block = AttentionBlock(self.input_size)\n",
    "\n",
    "        self.FC_head = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.input_size, self.input_size)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            # ('fc2', nn.Linear(1024, self.input_size)),\n",
    "            # ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "\n",
    "        self.FC_output = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.input_size, 1)),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.word_embedding(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        output = mean_pooling(output, x)\n",
    "\n",
    "        if self.with_features:\n",
    "            output = torch.cat((output, x['features']), dim=1)\n",
    "\n",
    "        output = output.unsqueeze(1)\n",
    "        output = self.attention(output)\n",
    "        output = self.FC_output(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "        self.attention = SelfAttention(input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.attention(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x += identity\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5b886fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MOdel without features\n",
    "\n",
    "# class STSBertModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Sentence Semantic Similarity Bert model\n",
    "#     :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "#     :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "#     \"\"\"\n",
    "#     def __init__(self, seg_head=False, freeze_weights=False, input_size=768):\n",
    "#         super(STSBertModel, self).__init__()\n",
    "        \n",
    "#         self.word_embedding = embeddings_model\n",
    "#         self.cos_sim = nn.CosineSimilarity(dim=0)\n",
    "#         self.identity = nn.Identity()\n",
    "#         self.seg_head = seg_head\n",
    "#         self.input_size = input_size\n",
    "        \n",
    "#         self.attention = SelfAttention(self.input_size)\n",
    "#         self.FC_head = nn.Sequential(OrderedDict([\n",
    "#             ('fc1', nn.Linear(self.input_size, 1))\n",
    "#         ]))\n",
    "            \n",
    "#         if freeze_weights:\n",
    "#             self.freeze()\n",
    "    \n",
    "#     def freeze(self):\n",
    "#         for param in self.word_embedding.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     def cos_score(self, x):\n",
    "#         embedding_1 = x[0]\n",
    "#         embedding_2 = x[1]\n",
    "        \n",
    "#         res = self.cos_sim(embedding_1, embedding_2)\n",
    "# #         cos_score = torch.sigmoid(cos_score)\n",
    "\n",
    "#         return res\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         output = self.word_embedding(**x)\n",
    "#         output = mean_pooling(output, x)\n",
    "#         output = self.attention(output.unsqueeze(1))\n",
    "\n",
    "# #         output = self.attention(output[0])\n",
    "# #         output = torch.mean(output, dim=1)\n",
    "# #         output = torch.max(output, dim=1).values\n",
    "    \n",
    "#         if self.seg_head:            \n",
    "#             output = self.FC_head(output)\n",
    "        \n",
    "#         else:\n",
    "#             output = self.cos_score(output)\n",
    "#             output = (output * 6.1) - 1.8\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ad634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class STSBertModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Sentence Semantic Similarity Bert model\n",
    "#     :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "#     :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "#     \"\"\"\n",
    "#     def __init__(self, seg_head=False, freeze_weights=False, input_size=768):\n",
    "#         super(STSBertModel, self).__init__()\n",
    "        \n",
    "#         self.word_embedding = embeddings_model\n",
    "#         self.cos_sim = nn.CosineSimilarity(dim=0)\n",
    "#         self.identity = nn.Identity()\n",
    "#         self.seg_head = seg_head\n",
    "#         self.fc = nn.Sequential(OrderedDict([\n",
    "#             ('fc1', nn.Linear(input_size*2, 1024)),\n",
    "#             ('tanh', nn.Tanh()),\n",
    "#             ('fc2', nn.Linear(1024, 256)),\n",
    "#             ('tanh', nn.Tanh()),\n",
    "#             ('output', nn.Linear(256, 1)),\n",
    "#         ]))\n",
    "        \n",
    "        \n",
    "#         if seg_head:\n",
    "# #             self.FC_head = SbertHead(inputs=768)\n",
    "#             self.FC_head = nn.Sequential(OrderedDict([\n",
    "# #                 ('dropout1', nn.Dropout(0.2)),\n",
    "#                 ('fc_input', nn.Linear(input_size, 1024)),\n",
    "#                 ('relu1', nn.ReLU()),\n",
    "#                 ('fc1', nn.Linear(1024,1024)),\n",
    "# #                 ('dropout2', nn.Dropout(0.2)),\n",
    "# #                 ('batch_norm1', nn.BatchNorm1d(1024)),\n",
    "#                 ('relu2', nn.ReLU()),\n",
    "#                 ('fc2', nn.Linear(1024, 512)),\n",
    "#                 ('relu3', nn.ReLU()),\n",
    "# #                 ('dropout2', nn.Dropout(0.2)),\n",
    "#                 ('fc3', nn.Linear(512, 512)),\n",
    "# #                 ('batch_norm2', nn.BatchNorm1d(512)),\n",
    "#                 ('relu4', nn.ReLU()),\n",
    "#                 ('fc4', nn.Linear(512, 256)),\n",
    "# #                 ('batch_norm3', nn.BatchNorm1d(256)),\n",
    "#                 ('relu5', nn.ReLU()),\n",
    "# #                 ('dropout3', nn.Dropout(0.2)),\n",
    "#                 ('fc5', nn.Linear(256, 64)),\n",
    "#                 ('relu6', nn.ReLU()),\n",
    "#                 ('fc_output', nn.Linear(64, 1))\n",
    "# #                 ('activation', nn.Sigmoid())\n",
    "#             ]))\n",
    "            \n",
    "#         if freeze_weights:\n",
    "#             self.freeze()\n",
    "    \n",
    "#     def freeze(self):\n",
    "#         for param in self.word_embedding.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def mean_pooling(outputs, batch) -> torch.tensor:\n",
    "        \n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         embeddings = outputs.last_hidden_state\n",
    "    \n",
    "#         mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "#         masked_embeddings = embeddings * mask\n",
    "    \n",
    "#         summed = torch.sum(masked_embeddings, 1)\n",
    "#         summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    \n",
    "#         mean_pooled = summed/summed_mask\n",
    "    \n",
    "#         return mean_pooled\n",
    "    \n",
    "#     def cos_score(self, x):\n",
    "#         embedding_1 = x[0]\n",
    "#         embedding_2 = x[1]\n",
    "#         emb = torch.concat((embedding_1, embedding_2))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         cos_score = self.fc(emb)\n",
    "        \n",
    "# #         cos_score = self.cos_sim(embedding_1, embedding_2)\n",
    "# #         cos_score = torch.sigmoid(cos_score)\n",
    "        \n",
    "#         return cos_score\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         output = self.word_embedding(**x)\n",
    "#         output = self.mean_pooling(output, x)\n",
    "# #         output = output[1]\n",
    "        \n",
    "#         if self.seg_head:\n",
    "#             output = self.FC_head(output)\n",
    "        \n",
    "#         else:\n",
    "#             output = self.cos_score(output)\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "475302d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Полносвязная модель - принимает тензор с эмбедингами текстов\n",
    "# # Выдает 1 число [0, 1]\n",
    "\n",
    "# class SbertHead(nn.Module):\n",
    "#     def __init__(self, inputs=1024):\n",
    "#         super(SbertHead, self).__init__()\n",
    "        \n",
    "#         self.FC_input = nn.Linear(inputs, 1024)\n",
    "#         self.FC_hidden_0 = nn.Linear(1024, 1024)\n",
    "#         self.FC_hidden_1 = nn.Linear(1024, 512)\n",
    "#         self.FC_hidden_2 = nn.Linear(512, 512)\n",
    "#         self.FC_hidden_3 = nn.Linear(512, 256)\n",
    "#         self.FC_hidden_4 = nn.Linear(256, 64)\n",
    "#         self.FC_output = nn.Linear(64, 1)\n",
    "#         self.activation = nn.Sigmoid()\n",
    "#         self.batch_norm = nn.BatchNorm1d(1024)\n",
    "#         self.batch_norm_2 = nn.BatchNorm1d(512)\n",
    "#         self.batch_norm_3 = nn.BatchNorm1d(64)\n",
    "\n",
    "#     def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \n",
    "#         x = F.relu(self.FC_input(x))\n",
    "#         x = F.relu(self.FC_hidden_0(x))\n",
    "#         x = self.batch_norm(x)\n",
    "#         x = F.relu(self.FC_hidden_1(x))\n",
    "#         x = F.relu(self.FC_hidden_2(x))\n",
    "#         x = self.batch_norm_2(x)\n",
    "#         x = F.relu(self.FC_hidden_3(x))\n",
    "#         x = F.relu(self.FC_hidden_4(x))\n",
    "#         x = self.batch_norm_3(x)\n",
    "        \n",
    "#         x = self.FC_output(x)\n",
    "#         x = self.activation(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34385c1",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ebf0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type = 'content'\n",
    "\n",
    "train_data = SentenseData(score=score_type)\n",
    "# test_data = SentenseData(test=True, score=score_type)\n",
    "# train_data = SentenseData(score=score_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d1f1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, drop_last=True)\n",
    "\n",
    "# test_loader = DataLoader(test_data,\n",
    "#                           batch_size=batch_size,\n",
    "#                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe34f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = SentenseData(summaries_train_path, prompts_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33505b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8*len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "\n",
    "# train_data, test_data = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# # torch.save(train_data, 'dataset\\TRAIN_DATA_split_1.pt')\n",
    "# # torch.save(test_data, 'dataset\\TEST_DATA_split_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cad57f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = torch.load('dataset\\TRAIN_DATA_split_1.pt')\n",
    "# test_data = torch.load('dataset\\TEST_DATA_split_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31a73c",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b693153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = STSBertModel(with_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c4b225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "STSBertModel(\n  (word_embedding): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (12): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (13): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (14): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (15): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (16): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (17): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (18): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (19): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (20): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (21): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (22): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n        (23): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (cos_score): CosineSimilarity()\n  (identity): Identity()\n  (attention): SelfAttention(\n    (query): Linear(in_features=1029, out_features=1029, bias=True)\n    (key): Linear(in_features=1029, out_features=1029, bias=True)\n    (value): Linear(in_features=1029, out_features=1029, bias=True)\n    (softmax): Softmax(dim=2)\n  )\n  (block): AttentionBlock(\n    (fc): Linear(in_features=1029, out_features=1029, bias=True)\n    (attention): SelfAttention(\n      (query): Linear(in_features=1029, out_features=1029, bias=True)\n      (key): Linear(in_features=1029, out_features=1029, bias=True)\n      (value): Linear(in_features=1029, out_features=1029, bias=True)\n      (softmax): Softmax(dim=2)\n    )\n    (relu): ReLU()\n    (norm): LayerNorm((1029,), eps=1e-05, elementwise_affine=True)\n  )\n  (FC_head): Sequential(\n    (fc1): Linear(in_features=1029, out_features=1029, bias=True)\n    (relu1): ReLU()\n  )\n  (FC_output): Sequential(\n    (fc1): Linear(in_features=1029, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.float()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f565158",
   "metadata": {},
   "outputs": [],
   "source": [
    " lr_head = 5e-05\n",
    "\n",
    " params = [\n",
    "        {'params': model.word_embedding.parameters()},\n",
    "        {'params': model.attention.parameters(), 'lr': lr_head},\n",
    "        {'params': model.block.parameters(), 'lr': lr_head},\n",
    "        {'params': model.FC_head.parameters(), 'lr': lr_head},\n",
    "        {'params': model.FC_output.parameters(), 'lr': lr_head}\n",
    "    ]\n",
    "\n",
    "Loss_func = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(params, lr=2e-06, weight_decay=1e-02)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.7)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb412653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1433/1433 [27:14<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Iter: 0, L_train: 0.45529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1433/1433 [24:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3, Iter: 0, L_train: 0.38083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1433/1433 [15:14<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3, Iter: 0, L_train: 0.34577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_model():\n",
    "\n",
    "    losses_epoch_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, position=0, leave=True):\n",
    "            targets = targets.float().to(device)\n",
    "            \n",
    "            if True:#score_type == 'wording':\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = torch.sqrt(Loss_func(outputs.squeeze(), targets))\n",
    "            else:\n",
    "                outputs = torch.stack([model(sentence.to(device)) for sentence in inputs])              \n",
    "                loss = torch.sqrt(Loss_func(outputs, targets))\n",
    "\n",
    "            \n",
    "            losses_epoch_test.append(loss.item())\n",
    "    \n",
    "    return losses_epoch_test\n",
    "\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "t0 = datetime.now()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    losses_train_per_epoch = []\n",
    "    \n",
    "    for j, batch in enumerate(tqdm(train_loader, position=0, leave=True), 1):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.float().to(device)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            if True:#score_type == 'wording':\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "                outputs = model(inputs)\n",
    "                loss = torch.sqrt(Loss_func(outputs.squeeze(), targets))\n",
    "            else:\n",
    "                outputs = torch.stack([model(sentence.to(device)) for sentence in inputs])           \n",
    "                loss = torch.sqrt(Loss_func(outputs, targets))\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        \n",
    "        scaler.update()\n",
    "        \n",
    "        losses_train_per_epoch.append(loss.item())\n",
    "        \n",
    "#         if j%20 == 0 or j == len(train_loader):\n",
    "            \n",
    "#             losses_test_per_epoch = validate_model()\n",
    "#             losses_test_mean = np.mean(losses_test_per_epoch)\n",
    "#             losses_train_mean = np.mean(losses_train_per_epoch)\n",
    "            \n",
    "#             losses_train.append(losses_train_mean)\n",
    "#             losses_test.append(losses_test_mean)\n",
    "            \n",
    "            \n",
    "#             if losses_test_mean == np.min(losses_test):\n",
    "#                 best_score = losses_test_mean\n",
    "#                 n_iter = len(losses_train)\n",
    "#                 torch.save(model.state_dict(), 'best__.pt')\n",
    "#                 print(f'Best saved, loss: {best_score:.5f}')\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f'Epoch: {i+1}/{n_epochs}, Iter: {len(losses_train)}, L_train: {np.mean(losses_train_per_epoch):.5f}')#,\\\n",
    "            #L_test: {losses_test_mean:.5f}') \n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#print(f\"Time elapsed:{datetime.now()-t0}, Best score:{best_score:.5f} at {n_iter} iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3164c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models\\Roberta_large_content_8.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
