{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7080d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\compat.py:11\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentence_transformers\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\__init__.py:11\u001B[0m\n\u001B[0;32m      7\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m4.3.1\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m     14\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgensim\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m logger\u001B[38;5;241m.\u001B[39mhandlers:  \u001B[38;5;66;03m# To ensure reload() doesn't add another one\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\parsing\\__init__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"This package contains functions to preprocess raw text\"\"\"\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PorterStemmer  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa:F401\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     preprocess_documents,\n\u001B[0;32m      6\u001B[0m     preprocess_string,\n\u001B[0;32m      7\u001B[0m     read_file,\n\u001B[0;32m      8\u001B[0m     read_files,\n\u001B[0;32m      9\u001B[0m     remove_stopwords,\n\u001B[0;32m     10\u001B[0m     split_alphanum,\n\u001B[0;32m     11\u001B[0m     stem_text,\n\u001B[0;32m     12\u001B[0m     strip_multiple_whitespaces,\n\u001B[0;32m     13\u001B[0m     strip_non_alphanum,\n\u001B[0;32m     14\u001B[0m     strip_numeric,\n\u001B[0;32m     15\u001B[0m     strip_punctuation,\n\u001B[0;32m     16\u001B[0m     strip_short,\n\u001B[0;32m     17\u001B[0m     strip_tags,\n\u001B[0;32m     18\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\parsing\\preprocessing.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstring\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mglob\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PorterStemmer\n\u001B[0;32m     30\u001B[0m STOPWORDS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m([\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msix\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjust\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mless\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbeing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindeed\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mover\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmove\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124manyway\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfour\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mown\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthrough\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124musing\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfifty\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhere\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmill\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124monly\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfind\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbefore\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mone\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwhose\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhow\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msomewhere\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmake\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124monce\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     59\u001B[0m ])\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28mopen\u001B[39m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m gensim_version\n\u001B[0;32m     41\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\__init__.py:34\u001B[0m\n\u001B[0;32m     31\u001B[0m logger\u001B[38;5;241m.\u001B[39maddHandler(logging\u001B[38;5;241m.\u001B[39mNullHandler())\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msmart_open_lib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28mopen\u001B[39m, parse_uri, smart_open, register_compressor  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[0;32m     36\u001B[0m _WARNING \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124msmart_open.s3_iter_bucket is deprecated and will stop functioning\u001B[39m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124min a future version. Please import iter_bucket from the smart_open.s3 module instead:\u001B[39m\n\u001B[0;32m     38\u001B[0m \n\u001B[0;32m     39\u001B[0m \u001B[38;5;124m    from smart_open.s3 import iter_bucket as s3_iter_bucket\u001B[39m\n\u001B[0;32m     40\u001B[0m \n\u001B[0;32m     41\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     42\u001B[0m _WARNED \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\smart_open_lib.py:35\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlocal_file\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mso_file\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompression\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mso_compression\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m doctools\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msmart_open\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transport\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m# For backwards compatibility and keeping old unit tests happy.\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\doctools.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transport\n\u001B[0;32m     23\u001B[0m PLACEHOLDER \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m    smart_open/doctools.py magic goes here\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextract_kwargs\u001B[39m(docstring):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\transport.py:106\u001B[0m\n\u001B[0;32m    104\u001B[0m register_transport(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmart_open.s3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    105\u001B[0m register_transport(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msmart_open.ssh\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 106\u001B[0m \u001B[43mregister_transport\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msmart_open.webhdfs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    108\u001B[0m SUPPORTED_SCHEMES \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28msorted\u001B[39m(_REGISTRY\u001B[38;5;241m.\u001B[39mkeys()))\n\u001B[0;32m    109\u001B[0m \u001B[38;5;124;03m\"\"\"The transport schemes that the local installation of ``smart_open`` supports.\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\transport.py:49\u001B[0m, in \u001B[0;36mregister_transport\u001B[1;34m(submodule)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(submodule, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 49\u001B[0m         submodule \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\importlib\\__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\smart_open\\webhdfs.py:20\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m     22\u001B[0m     MISSING_DEPS \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\__init__.py:45\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RequestsDependencyWarning\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m charset_normalizer_version\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\exceptions.py:9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mrequests.exceptions\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03mThis module contains the set of Requests' exceptions.\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m BaseHTTPError\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m CompatJSONDecodeError\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mRequestException\u001B[39;00m(\u001B[38;5;167;01mIOError\u001B[39;00m):\n\u001B[0;32m     13\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m    request.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\requests\\compat.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Pythons\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Syntax sugar.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\charset_normalizer\\__init__.py:23\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mCharset-Normalizer\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m:license: MIT, see LICENSE for more details.\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_fp, from_path, from_bytes, normalize\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m detect\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__, VERSION\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\charset_normalizer\\api.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m     PathLike \u001B[38;5;241m=\u001B[39m Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mos.PathLike[str]\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mess_ratio\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharsetMatches, CharsetMatch\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m warn\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gensim\n",
    "import torch\n",
    "import sentence_transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, pipeline, BartModel\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoConfig\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8ee8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c88ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e55764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_model():\n",
    "    configuration = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    configuration.hidden_dropout_prob = 0.0\n",
    "    configuration.attention_probs_dropout_prob = 0.0\n",
    "    configuration.classifier_dropout = 0.0\n",
    "    \n",
    "    return configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c472860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ROBERTa model\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "embeddings_model = RobertaModel.from_pretrained(model_name, config=config_model())\n",
    "summarizer = pipeline(\"summarization\", model=\"bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05e4c1",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fee86d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_path = r\"dataset\\summaries_train.csv\"\n",
    "summaries_test_path = r\"dataset\\summaries_test.csv\"\n",
    "prompts_train_path = r\"dataset\\prompts_train.csv\"\n",
    "prompts_test_path = r\"dataset\\prompts_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f3f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normilizing score to values 0 to 1\"\"\"\n",
    "    df -= np.min(df)\n",
    "    df /= df.max()\n",
    "    print('Normilized' if df.min() == 0.0 and df.max() == 1.0 else 'NormError:wrong values')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def short_text(text, max_length=512) -> str:\n",
    "    if len(text.split()) > 700:\n",
    "        middle_point = text[len(text)//2:].index('.') + len(text)//2 + 1\n",
    "        text_pt1 = text[:middle_point]\n",
    "        text_pt2 = text[middle_point:]\n",
    "        \n",
    "        text = short_text(text_pt1) + short_text(text_pt2)\n",
    "        \n",
    "    summarized = summarizer(text, max_length=max_length, min_length=300, do_sample=False)\n",
    "    \n",
    "    return summarized[0]['summary_text']\n",
    "\n",
    "def moving_average(array: np.array, betta=0.9) -> np.array:\n",
    "    \"\"\"\n",
    "    Computing moving average with bias correction.\n",
    "    \"\"\"\n",
    "    \n",
    "    V = 0\n",
    "    average_array = np.zeros(len(array))\n",
    "    for i in range(len(array)):\n",
    "        V = betta * V + (1 - betta) * array[i]\n",
    "        average_array[i] = V/(1 - pow(betta, i+1))\n",
    "        \n",
    "    return average_array\n",
    "\n",
    "\n",
    "\n",
    "def mean_pooling(outputs, batch) -> torch.tensor:\n",
    "    \n",
    "    attention_mask = batch['attention_mask']\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    \n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    \n",
    "    mean_pooled = summed/summed_mask\n",
    "    \n",
    "    return mean_pooled\n",
    "\n",
    "def normilize(df_column: pd.DataFrame):\n",
    "    \n",
    "    df_column = (df_column - df_column.mean())/df_column.std()\n",
    "    \n",
    "    return df_column\n",
    "\n",
    "def get_stat_features(df, text_col=\"text\"):\n",
    "    \n",
    "    df[\"num_unique_words\"] = normilize(df[text_col].apply(lambda x: len(set(x.split()))))\n",
    "    df[\"num_words\"] = normilize(df[text_col].apply(lambda x: len(x.split())))\n",
    "    df[\"num_sentences\"] = normilize(df[text_col].apply(lambda x: len(x.split('.'))))\n",
    "    \n",
    "    df[\"syntax_count\"] = normilize(df[text_col].apply(lambda x: x.count(\",\") \n",
    "                                                      + x.count(\"-\") + x.count(\";\") + x.count(\":\")))\n",
    "    df['smog_index'] = normilize(df[text_col].apply(lambda x: textstat.smog_index(x)))\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b6081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenseData(Dataset):\n",
    "    \"\"\"\n",
    "    :params: path to csv file with summaries, path to csv file with prompts, 'score' param\\\n",
    "    defines which score is used: content/wording\n",
    "    \"\"\"\n",
    "    def __init__(self, summaries_path=summaries_train_path,\n",
    "                 prompts_path=prompts_train_path,\n",
    "                 score='wording', test=False, fold_test='ebad26'):\n",
    "        self.test = test\n",
    "        self.score_type = score\n",
    "        self.fold_test = fold_test\n",
    "        \n",
    "        with open(summaries_path, encoding='utf-8') as f:\n",
    "            summaries = pd.read_csv(f)\n",
    "            summaries = get_stat_features(summaries)\n",
    "            \n",
    "            self.summaries = summaries\n",
    "            \n",
    "        with open(prompts_path, encoding='utf-8') as f:\n",
    "            prompts = pd.read_csv(f)\n",
    "#             prompts = prompts[prompts.prompt_id != '3b9047'].reset_index(drop=True)\n",
    "            self.prompts = prompts\n",
    "        \n",
    "#         self.summaries.content = norm_score(self.summaries.content)\n",
    "#         self.summaries.wording = norm_score(self.summaries.wording)\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__summaries)\n",
    "    \n",
    "    @property\n",
    "    def summaries(self):\n",
    "        return self.__summaries\n",
    "    \n",
    "    @summaries.setter\n",
    "    def summaries(self, df):\n",
    "        if self.test:\n",
    "            self.__summaries = df[df.prompt_id == self.fold_test].reset_index(drop=True)\n",
    "            \n",
    "        else:\n",
    "            self.__summaries = df[df.prompt_id != self.fold_test].reset_index(drop=True)\n",
    "    \n",
    "    @property\n",
    "    def prompts(self):\n",
    "        return self.__prompts\n",
    "    \n",
    "    @prompts.setter\n",
    "    def prompts(self, file):\n",
    "        self.__prompts = file\n",
    "#         for i, text in enumerate(self.__prompts.prompt_text):\n",
    "#             if len(text.split()) > 600:\n",
    "#                 self.__prompts.prompt_text[i] = short_text(text)\n",
    "    \n",
    "    def get_batch_text(self, index):\n",
    "        summary_text = self.summaries.text[index]\n",
    "        \n",
    "        if True:#self.score_type == 'wording':\n",
    "            res = tokenizer(summary_text, padding='max_length', return_tensors='pt', truncation=True)\n",
    "            res = {k:val.squeeze() for k, val in res.items()}\n",
    "            res['features'] = torch.tensor([self.summaries['num_unique_words'][index], self.summaries['num_words'][index],\n",
    "                                  self.summaries['num_sentences'][index],self.summaries['syntax_count'][index],\n",
    "                                  self.summaries['smog_index'][index]], dtype=torch.float32)\n",
    "            \n",
    "            return res\n",
    "            \n",
    "        prompt_text = self.prompts.prompt_text[self.prompts.prompt_id ==\n",
    "                                               self.summaries.prompt_id[index]].item().replace('\\n','')\n",
    "        \n",
    "        return tokenizer([summary_text, prompt_text], padding='max_length', return_tensors='pt', truncation=True)\n",
    "    \n",
    "    def get_score(self, index):\n",
    "        if self.score_type == 'content':\n",
    "            score = self.summaries.content[index]\n",
    "            \n",
    "        elif self.score_type == 'wording':\n",
    "            score = self.summaries.wording[index]\n",
    "        \n",
    "        return torch.tensor(score)\n",
    "        \n",
    "    def __getitem__(self, index) -> torch.tensor:\n",
    "\n",
    "        batch_text = self.get_batch_text(index)\n",
    "        batch_score = self.get_score(index)\n",
    "        \n",
    "        return batch_text, batch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9bc8f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SentenseData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a91a43ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c99df5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data,\n",
    "                           batch_size=3,\n",
    "                           shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25f77cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    shme = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "969250b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shme[0]['attention_mask'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a67e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embeddings_model(input_ids=shme[0]['input_ids'], attention_mask=shme[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad5ec163",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mean_pooling(output, shme[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a575ce48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8927,  1.4800,  0.7691,  0.8581,  1.0899],\n",
       "        [-0.2083, -0.4264, -0.2766, -0.0857,  1.0288],\n",
       "        [-0.5471, -0.5946, -0.7995, -0.3216, -1.2527]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shme[0]['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bfe91e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6baddcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((output, shme[0]['features']), dim=1).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5b663",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476829dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STSBertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence Semantic Similarity Bert model\n",
    "    :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "    :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "    \"\"\"\n",
    "    def __init__(self, with_features=False, input_size=768):\n",
    "        super(STSBertModel, self).__init__()\n",
    "        \n",
    "        self.word_embedding = RobertaModel.from_pretrained(model_name, config=config_model())\n",
    "        self.cos_score = nn.CosineSimilarity(dim=0)\n",
    "        self.identity = nn.Identity()\n",
    "        self.input_size = input_size\n",
    "        self.with_features = with_features\n",
    "        \n",
    "        if with_features:\n",
    "            self.n_features = 5\n",
    "            self.input_size += self.n_features\n",
    "        \n",
    "        self.attention = SelfAttention(self.input_size)\n",
    "    \n",
    "        self.FC_head = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.input_size, 512)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(512, 512)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(512, 256)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(256, 64)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('fc5', nn.Linear(64, 1)),\n",
    "            ('identity', nn.Identity())\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.word_embedding(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        output = mean_pooling(output, x)\n",
    "        \n",
    "        if self.with_features:            \n",
    "            output = torch.cat((output, x['features']), dim=1)\n",
    "        \n",
    "        output = self.attention(output.unsqueeze(1))\n",
    "        output = self.FC_head(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018138e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c83e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class STSBertModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Sentence Semantic Similarity Bert model\n",
    "#     :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "#     :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "#     \"\"\"\n",
    "#     def __init__(self, seg_head=False, freeze_weights=False, input_size=768):\n",
    "#         super(STSBertModel, self).__init__()\n",
    "        \n",
    "#         self.word_embedding = RobertaModel.from_pretrained('roberta-base', config=config_model())\n",
    "#         self.cos_sim = nn.CosineSimilarity(dim=0)\n",
    "#         self.identity = nn.Identity()\n",
    "#         self.seg_head = seg_head\n",
    "#         self.fc = nn.Sequential(OrderedDict([\n",
    "#             ('fc1', nn.Linear(input_size*2, 1024)),\n",
    "#             ('tanh', nn.Tanh()),\n",
    "#             ('fc2', nn.Linear(1024, 256)),\n",
    "#             ('tanh', nn.Tanh()),\n",
    "#             ('output', nn.Linear(256, 1)),\n",
    "#         ]))\n",
    "        \n",
    "        \n",
    "#         if seg_head:\n",
    "# #             self.FC_head = SbertHead(inputs=768)\n",
    "#             self.FC_head = nn.Sequential(OrderedDict([\n",
    "# #                 ('dropout1', nn.Dropout(0.2)),\n",
    "#                 ('fc_input', nn.Linear(input_size, 1024)),\n",
    "#                 ('relu1', nn.ReLU()),\n",
    "#                 ('fc1', nn.Linear(1024,1024)),\n",
    "# #                 ('dropout2', nn.Dropout(0.2)),\n",
    "# #                 ('batch_norm1', nn.BatchNorm1d(1024)),\n",
    "#                 ('relu2', nn.ReLU()),\n",
    "#                 ('fc2', nn.Linear(1024, 512)),\n",
    "#                 ('relu3', nn.ReLU()),\n",
    "# #                 ('dropout2', nn.Dropout(0.2)),\n",
    "#                 ('fc3', nn.Linear(512, 512)),\n",
    "# #                 ('batch_norm2', nn.BatchNorm1d(512)),\n",
    "#                 ('relu4', nn.ReLU()),\n",
    "#                 ('fc4', nn.Linear(512, 256)),\n",
    "# #                 ('batch_norm3', nn.BatchNorm1d(256)),\n",
    "#                 ('relu5', nn.ReLU()),\n",
    "# #                 ('dropout3', nn.Dropout(0.2)),\n",
    "#                 ('fc5', nn.Linear(256, 64)),\n",
    "#                 ('relu6', nn.ReLU()),\n",
    "#                 ('fc_output', nn.Linear(64, 1))\n",
    "# #                 ('activation', nn.Sigmoid())\n",
    "#             ]))\n",
    "            \n",
    "#         if freeze_weights:\n",
    "#             self.freeze()\n",
    "    \n",
    "#     def freeze(self):\n",
    "#         for param in self.word_embedding.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def mean_pooling(outputs, batch) -> torch.tensor:\n",
    "        \n",
    "#         attention_mask = batch['attention_mask']\n",
    "#         embeddings = outputs.last_hidden_state\n",
    "    \n",
    "#         mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "#         masked_embeddings = embeddings * mask\n",
    "    \n",
    "#         summed = torch.sum(masked_embeddings, 1)\n",
    "#         summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    \n",
    "#         mean_pooled = summed/summed_mask\n",
    "    \n",
    "#         return mean_pooled\n",
    "    \n",
    "#     def cos_score(self, x):\n",
    "#         embedding_1 = x[0]\n",
    "#         embedding_2 = x[1]\n",
    "#         emb = torch.concat((embedding_1, embedding_2))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         cos_score = self.fc(emb)\n",
    "        \n",
    "# #         cos_score = self.cos_sim(embedding_1, embedding_2)\n",
    "# #         cos_score = torch.sigmoid(cos_score)\n",
    "        \n",
    "#         return cos_score\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         output = self.word_embedding(**x)\n",
    "#         output = self.mean_pooling(output, x)\n",
    "# #         output = output[1]\n",
    "        \n",
    "#         if self.seg_head:\n",
    "#             output = self.FC_head(output)\n",
    "        \n",
    "#         else:\n",
    "#             output = self.cos_score(output)\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_type = 'wording'\n",
    "# batch_size = 15\n",
    "\n",
    "# test_data = SentenseData(test=True, score=score_type)\n",
    "# train_data = SentenseData(score=score_type)\n",
    "\n",
    "# train_loader = DataLoader(train_data,\n",
    "#                            batch_size=batch_size,\n",
    "#                            shuffle=True)\n",
    "\n",
    "# test_loader = DataLoader(test_data,\n",
    "#                           batch_size=batch_size,\n",
    "#                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd3ae4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = STSBertModel(seg_head=True, freeze_weights=False)\n",
    "\n",
    "# model.float()\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d3cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss_func = nn.MSELoss()\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-06, weight_decay=2e-04)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, gamma=0.5)\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf62655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, loss_func):\n",
    "\n",
    "    losses_epoch_test = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            targets = targets.float().to(device)\n",
    "            if True:\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "            else:\n",
    "                outputs = torch.stack([model(sentence.to(device)) for sentence in inputs])\n",
    "\n",
    "\n",
    "            loss = torch.sqrt(loss_func(outputs.squeeze(), targets))\n",
    "\n",
    "            \n",
    "            losses_epoch_test.append(loss.item())\n",
    "    \n",
    "    return losses_epoch_test\n",
    "\n",
    "def train_model(n_epochs, lr, weight_decay, n_fold):\n",
    "    \n",
    "    model = STSBertModel(with_features=True)\n",
    "    \n",
    "    model.float()\n",
    "    model.to(device)\n",
    "    \n",
    "    params = [\n",
    "        {'params': model.word_embedding.parameters()},\n",
    "        {'params': model.attention.parameters(), 'lr': 1e-04},\n",
    "        {'params': model.FC_head.parameters(), 'lr': 1e-04}\n",
    "    ]\n",
    "    \n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, gamma=0.5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "    \n",
    "        losses_train_per_epoch = []\n",
    "    \n",
    "        for j, batch in enumerate(tqdm(train_loader, position=0, leave=True), 1):\n",
    "            inputs, targets = batch\n",
    "            targets = targets.float().to(device)\n",
    "        \n",
    "            model.train()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = torch.sqrt(loss_func(outputs.squeeze(), targets))\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "        \n",
    "            scaler.update()\n",
    "        \n",
    "            losses_train_per_epoch.append(loss.item())\n",
    "        \n",
    "            if j%15 == 0 or j == len(train_loader):\n",
    "            \n",
    "                losses_test_per_epoch = validate_model(model, loss_func)\n",
    "                losses_test_mean = np.mean(losses_test_per_epoch)\n",
    "                losses_train_mean = np.mean(losses_train_per_epoch)\n",
    "            \n",
    "                losses_train.append(losses_train_mean)\n",
    "                losses_test.append(losses_test_mean)\n",
    "            \n",
    "                if losses_test_mean == np.min(losses_test):\n",
    "                    best_score = losses_test_mean\n",
    "                    n_iter = len(losses_train)\n",
    "                    torch.save(model.state_dict(), r'models\\cv\\best__fold_{}.pt'.format(n_fold))\n",
    "                    print(f'Best saved, loss: {best_score:.5f}')\n",
    "                \n",
    "                print(f'Fold# {n_fold}, Epoch: {i+1}/{n_epochs}, L_train: {losses_train_mean:.5f},\\\n",
    "                L_test: {losses_test_mean:.5f}') \n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Time elapsed:{datetime.now()-t0}, Best score:{best_score:.5f}\")\n",
    "    \n",
    "    return best_score, n_iter, losses_test, losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2de476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc74e7f",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54030204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaders_init(score_type, batch_size, fold):\n",
    "    \n",
    "    test_data = SentenseData(test=True, score=score_type, fold_test=fold)\n",
    "    train_data = SentenseData(score=score_type, fold_test=fold)\n",
    "    \n",
    "    train_loader = DataLoader(train_data,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(test_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82f73dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_set(ax):\n",
    "    \n",
    "    ax.plot(test_losses, label='test')\n",
    "    ax.plot(train_losses, label='train')\n",
    "    ax.scatter(n_iter - 1, best_score, label='min')\n",
    "    ax.set_title(f'#{i}, min: {best_score:.5f}', loc='left')\n",
    "    ax.xaxis.set_major_locator(ticker.LinearLocator(n_epochs + 1))\n",
    "    ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05444cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|                                                        | 100/340 [00:37<21:34,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.59299\n",
      "Fold# 1, Epoch: 1/5, L_train: 0.76454,                L_test: 0.59299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 200/340 [01:14<12:18,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/5, L_train: 0.70534,                L_test: 0.59735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 300/340 [01:50<03:31,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/5, L_train: 0.68006,                L_test: 0.61006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 340/340 [02:16<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.58367\n",
      "Fold# 1, Epoch: 1/5, L_train: 0.67486,                L_test: 0.58367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/340 [00:36<21:07,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 2/5, L_train: 0.59780,                L_test: 0.60477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 200/340 [01:13<12:18,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 2/5, L_train: 0.59331,                L_test: 0.64144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 300/340 [01:50<03:32,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 2/5, L_train: 0.58479,                L_test: 0.63864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 340/340 [02:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 2/5, L_train: 0.57894,                L_test: 0.62049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/340 [00:36<21:06,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 3/5, L_train: 0.53232,                L_test: 0.63339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 200/340 [01:13<12:19,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 3/5, L_train: 0.53106,                L_test: 0.62436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 300/340 [01:50<03:31,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 3/5, L_train: 0.53512,                L_test: 0.62974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 340/340 [02:15<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 3/5, L_train: 0.53474,                L_test: 0.64374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/340 [00:36<21:07,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 4/5, L_train: 0.49416,                L_test: 0.67997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 200/340 [01:14<12:37,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.53988\n",
      "Fold# 1, Epoch: 4/5, L_train: 0.50765,                L_test: 0.53988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 300/340 [01:50<03:31,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 4/5, L_train: 0.51016,                L_test: 0.73795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 340/340 [02:15<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 4/5, L_train: 0.50802,                L_test: 0.59054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/340 [00:36<21:09,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 5/5, L_train: 0.49540,                L_test: 0.72137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 200/340 [01:13<12:19,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 5/5, L_train: 0.47621,                L_test: 0.63909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 300/340 [01:50<03:31,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 5/5, L_train: 0.46993,                L_test: 0.71714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 340/340 [02:15<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 5/5, L_train: 0.47096,                L_test: 0.69909\n",
      "Time elapsed:0:11:19.141757, Best score:0.53988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\shmak\\AppData\\Local\\Temp\\ipykernel_25720\\2503778341.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|                                                        | 100/343 [00:36<21:27,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.95058\n",
      "Fold# 2, Epoch: 1/5, L_train: 0.69231,                L_test: 0.95058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/343 [01:13<12:22,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 1/5, L_train: 0.64834,                L_test: 0.95240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/343 [01:50<03:48,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.86919\n",
      "Fold# 2, Epoch: 1/5, L_train: 0.62740,                L_test: 0.86919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 343/343 [02:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.80183\n",
      "Fold# 2, Epoch: 1/5, L_train: 0.62268,                L_test: 0.80183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/343 [00:36<21:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 2/5, L_train: 0.54030,                L_test: 0.94194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/343 [01:13<12:22,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 2/5, L_train: 0.54292,                L_test: 0.87150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/343 [01:49<03:42,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 2/5, L_train: 0.54124,                L_test: 0.92012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 343/343 [02:14<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 2/5, L_train: 0.53880,                L_test: 0.84681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 101/343 [00:36<14:53,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 3/5, L_train: 0.50034,                L_test: 0.91416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/343 [01:13<12:21,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 3/5, L_train: 0.50820,                L_test: 0.86858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/343 [01:49<03:42,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 3/5, L_train: 0.50548,                L_test: 0.87782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 343/343 [02:14<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 3/5, L_train: 0.50333,                L_test: 0.84631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/343 [00:37<21:21,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 4/5, L_train: 0.46322,                L_test: 0.84670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/343 [01:13<12:25,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 4/5, L_train: 0.47417,                L_test: 0.84333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 301/343 [01:50<02:35,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 4/5, L_train: 0.47616,                L_test: 0.87413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 343/343 [02:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 4/5, L_train: 0.47613,                L_test: 0.81361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/343 [00:37<21:41,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.77862\n",
      "Fold# 2, Epoch: 5/5, L_train: 0.43901,                L_test: 0.77862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/343 [01:14<12:41,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.76830\n",
      "Fold# 2, Epoch: 5/5, L_train: 0.44085,                L_test: 0.76830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/343 [01:50<03:43,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 2, Epoch: 5/5, L_train: 0.43823,                L_test: 0.85116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 343/343 [02:16<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.76157\n",
      "Fold# 2, Epoch: 5/5, L_train: 0.44069,                L_test: 0.76157\n",
      "Time elapsed:0:11:17.460307, Best score:0.76157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\shmak\\AppData\\Local\\Temp\\ipykernel_25720\\2503778341.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|                                                            | 100/404 [00:29<15:27,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.85112\n",
      "Fold# 3, Epoch: 1/5, L_train: 0.74153,                L_test: 0.85112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 200/404 [00:58<09:56,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 1/5, L_train: 0.67932,                L_test: 0.96075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 300/404 [01:27<05:17,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.84434\n",
      "Fold# 3, Epoch: 1/5, L_train: 0.65834,                L_test: 0.84434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 400/404 [01:57<00:11,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 1/5, L_train: 0.64043,                L_test: 0.90667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 404/404 [02:07<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.83474\n",
      "Fold# 3, Epoch: 1/5, L_train: 0.64027,                L_test: 0.83474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                            | 100/404 [00:29<15:32,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.79516\n",
      "Fold# 3, Epoch: 2/5, L_train: 0.56441,                L_test: 0.79516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 200/404 [00:59<10:12,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 2/5, L_train: 0.54971,                L_test: 0.79546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 300/404 [01:28<05:06,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 2/5, L_train: 0.54517,                L_test: 0.83928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 400/404 [01:58<00:12,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.77133\n",
      "Fold# 3, Epoch: 2/5, L_train: 0.54379,                L_test: 0.77133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 404/404 [02:08<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 2/5, L_train: 0.54281,                L_test: 0.85319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                            | 100/404 [00:29<15:31,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.75803\n",
      "Fold# 3, Epoch: 3/5, L_train: 0.50700,                L_test: 0.75803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 200/404 [00:59<10:26,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.70137\n",
      "Fold# 3, Epoch: 3/5, L_train: 0.50180,                L_test: 0.70137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 300/404 [01:28<05:09,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 3/5, L_train: 0.50535,                L_test: 0.78837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 400/404 [01:57<00:11,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 3/5, L_train: 0.50864,                L_test: 0.77399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 404/404 [02:07<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 3/5, L_train: 0.50809,                L_test: 0.70294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                            | 101/404 [00:29<11:04,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.69404\n",
      "Fold# 3, Epoch: 4/5, L_train: 0.48043,                L_test: 0.69404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 200/404 [00:58<09:57,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 4/5, L_train: 0.48433,                L_test: 0.74438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 300/404 [01:27<05:19,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.69090\n",
      "Fold# 3, Epoch: 4/5, L_train: 0.48192,                L_test: 0.69090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 400/404 [01:57<00:11,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 4/5, L_train: 0.47749,                L_test: 0.74878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 404/404 [02:06<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 4/5, L_train: 0.47739,                L_test: 0.71954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                            | 100/404 [00:29<14:46,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 5/5, L_train: 0.44186,                L_test: 0.69317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 200/404 [00:58<10:01,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 5/5, L_train: 0.45098,                L_test: 0.75706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 300/404 [01:27<05:04,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 5/5, L_train: 0.45176,                L_test: 0.70975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 400/404 [01:56<00:11,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 5/5, L_train: 0.45168,                L_test: 0.70752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 404/404 [02:06<00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 3, Epoch: 5/5, L_train: 0.45178,                L_test: 0.70371\n",
      "Time elapsed:0:10:36.223476, Best score:0.69090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\shmak\\AppData\\Local\\Temp\\ipykernel_25720\\2503778341.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 29%|                                                        | 100/344 [00:37<21:39,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.67078\n",
      "Fold# 4, Epoch: 1/5, L_train: 0.75498,                L_test: 0.67078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/344 [01:14<12:45,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.62521\n",
      "Fold# 4, Epoch: 1/5, L_train: 0.71601,                L_test: 0.62521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/344 [01:51<03:54,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.61713\n",
      "Fold# 4, Epoch: 1/5, L_train: 0.68285,                L_test: 0.61713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 344/344 [02:16<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 1/5, L_train: 0.67379,                L_test: 0.62227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/344 [00:37<21:49,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.61640\n",
      "Fold# 4, Epoch: 2/5, L_train: 0.59737,                L_test: 0.61640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/344 [01:13<12:28,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 2/5, L_train: 0.57960,                L_test: 0.62284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/344 [01:50<03:53,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.61522\n",
      "Fold# 4, Epoch: 2/5, L_train: 0.59003,                L_test: 0.61522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 344/344 [02:16<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 2/5, L_train: 0.58900,                L_test: 0.64566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/344 [00:36<21:08,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 3/5, L_train: 0.53542,                L_test: 0.61683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/344 [01:13<12:44,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.58757\n",
      "Fold# 4, Epoch: 3/5, L_train: 0.54168,                L_test: 0.58757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/344 [01:49<03:47,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 3/5, L_train: 0.54662,                L_test: 0.61968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 344/344 [02:15<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 3/5, L_train: 0.54795,                L_test: 0.59979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/344 [00:36<21:03,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 4/5, L_train: 0.49968,                L_test: 0.60405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/344 [01:13<12:26,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 4/5, L_train: 0.50949,                L_test: 0.63097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/344 [01:50<03:54,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.58463\n",
      "Fold# 4, Epoch: 4/5, L_train: 0.51230,                L_test: 0.58463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 344/344 [02:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 4/5, L_train: 0.51270,                L_test: 0.59484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                        | 100/344 [00:36<21:03,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 5/5, L_train: 0.48952,                L_test: 0.59773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                 | 200/344 [01:13<12:38,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 5/5, L_train: 0.49002,                L_test: 0.59655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 300/344 [01:50<03:49,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 4, Epoch: 5/5, L_train: 0.48588,                L_test: 0.62019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 344/344 [02:16<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.57768\n",
      "Fold# 4, Epoch: 5/5, L_train: 0.48043,                L_test: 0.57768\n",
      "Time elapsed:0:11:20.876812, Best score:0.57768\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\shmak\\AppData\\Local\\Temp\\ipykernel_25720\\2503778341.py:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "  File \"C:\\Users\\shmak\\AppData\\Local\\Temp\\ipykernel_25720\\3764877995.py\", line 39, in <module>\n",
      "    fig.suptitle(f'{mean_score:.5f}', loc='left')\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\figure.py\", line 381, in suptitle\n",
      "    return self._suplabels(t, info, **kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\figure.py\", line 360, in _suplabels\n",
      "    sup = self.text(x, y, t, **kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\figure.py\", line 1166, in text\n",
      "    text = Text(x=x, y=y, text=s, **effective_kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\", line 454, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\text.py\", line 186, in __init__\n",
      "    self.update(kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\text.py\", line 198, in update\n",
      "    super().update(kwargs)\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\artist.py\", line 1176, in update\n",
      "    return self._update_props(\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\matplotlib\\artist.py\", line 1160, in _update_props\n",
      "    raise AttributeError(\n",
      "AttributeError: 'Text' object has no property 'loc'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 82, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2052, in showtraceback\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 838, in get_records\n",
      "  File \"C:\\Users\\shmak\\miniconda3\\envs\\tf\\lib\\site-packages\\pygments\\styles\\__init__.py\", line 84, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'default', though it should be builtin.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def cross_validation():\n",
    "with open(prompts_train_path, encoding='utf-8') as f:\n",
    "    prompts = pd.read_csv(f)\n",
    "#     prompts = prompts[prompts.prompt_id != '3b9047'].reset_index(drop=True)\n",
    "    \n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "folds_list = list(prompts.prompt_id)\n",
    "score_type = 'wording'\n",
    "batch_size = 15\n",
    "lr = 2e-06\n",
    "weight_decay = 2e-02\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "scores = []\n",
    "\n",
    "fig, axs = plt.subplots(1, len(folds_list), figsize=(12,4), layout='tight')\n",
    "\n",
    "for i, fold in enumerate(folds_list, 1):\n",
    "    train_loader, test_loader = loaders_init(score_type, batch_size, fold)\n",
    "    \n",
    "    best_score, n_iter, test_losses, train_losses = train_model(n_epochs, lr, weight_decay, i)\n",
    "    \n",
    "    scores.append(best_score)\n",
    "    \n",
    "    axs[i-1] = axis_set(axs[i-1])\n",
    "    \n",
    "#     plt.subplot(1, 4, i)\n",
    "#     plt.plot(test_losses, label='test')\n",
    "#     plt.plot(train_losses, label='train')\n",
    "#     plt.scatter(n_iter - 1, best_score, label='min')\n",
    "#     plt.legend()\n",
    "#     plt.title(f'#{i}, min: {best_score:.5f}')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "mean_score = np.mean(scores)\n",
    "\n",
    "fig.suptitle(f'{mean_score:.5f}')\n",
    "fig.savefig('res_figure.png', dpi=200)\n",
    "\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1f37418",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.suptitle(f'{mean_score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59b578c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('res_figure.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, len(fold_list), figsize=(12,4), layout='tight')\n",
    "\n",
    "# axs[i].plot(test_losses, label='test')\n",
    "# axs[i].plot(train_losses, label='train')\n",
    "# axs[i].scatter(n_iter - 1, best_score, label='min')\n",
    "# axs[i].set_title(f'#{i}, min: {best_score:.5f}', loc=left)\n",
    "# axs[i].xaxis.set_major_locator(ticker.LinearLocator(n_epochs + 1))\n",
    "# axs[i].xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256ad3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mscores\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15305588",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c676e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['39c16e', '814d6b', 'ebad26']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
