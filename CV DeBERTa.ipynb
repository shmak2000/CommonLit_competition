{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import sentence_transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "%matplotlib inline\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, pipeline, BartModel\n",
    "from transformers import RobertaTokenizer, RobertaModel, AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "from DeBERTa import deberta\n",
    "from collections import OrderedDict\n",
    "\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_name = 'base'\n",
    "size = 768"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def config_debert():\n",
    "    configuration = deberta.ModelConfig()\n",
    "    configuration = configuration.from_json_file(r\"C:\\Users\\shmak\\.~DeBERTa\\assets\\latest\\deberta-base\\model_config.json\")\n",
    "\n",
    "    configuration.attention_probs_dropout_prob = 0.0\n",
    "    configuration.hidden_dropout_prob = 0.0\n",
    "\n",
    "    return configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = RobertaTokenizer.from_pretrained()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2023 15:38:44|INFO|logging|00| Loaded pretrained model file C:\\Users\\shmak\\.~DeBERTa/assets/latest/deberta-base\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# DeBERTa model\n",
    "#\n",
    "# vocab_path, vocab_type = deberta.load_vocab(pretrained_id='large')\n",
    "# tokenizer = deberta.tokenizers[vocab_type](vocab_path)\n",
    "\n",
    "embeddings_model = deberta.DeBERTa(config=config_debert(), pre_trained=model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "summaries_train_path = r\"dataset\\summaries_train_merged.csv\"\n",
    "summaries_test_path = r\"dataset\\summaries_test.csv\"\n",
    "prompts_train_path = r\"dataset\\prompts_train.csv\"\n",
    "prompts_test_path = r\"dataset\\prompts_test.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def norm_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normilizing score to values 0 to 1\"\"\"\n",
    "    df -= np.min(df)\n",
    "    df /= df.max()\n",
    "    print('Normilized' if df.min() == 0.0 and df.max() == 1.0 else 'NormError:wrong values')\n",
    "\n",
    "    return df\n",
    "\n",
    "def short_text(text, max_length=200) -> str:\n",
    "    if len(text.split()) > 700:\n",
    "        middle_point = text[len(text)//2:].index('.') + len(text)//2 + 1\n",
    "        text_pt1 = text[:middle_point]\n",
    "        text_pt2 = text[middle_point:]\n",
    "\n",
    "        text = short_text(text_pt1) + short_text(text_pt2)\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=\"bart-large-cnn\")\n",
    "\n",
    "    # summarizer.model.to(device)\n",
    "\n",
    "    summarized = summarizer(text, max_length=max_length, min_length=80, do_sample=False)\n",
    "\n",
    "    return summarized[0]['summary_text']\n",
    "\n",
    "def moving_average(array: np.array, betta=0.9) -> np.array:\n",
    "    \"\"\"\n",
    "    Computing moving average with bias correction.\n",
    "    \"\"\"\n",
    "\n",
    "    V = 0\n",
    "    average_array = np.zeros(len(array))\n",
    "    for i in range(len(array)):\n",
    "        V = betta * V + (1 - betta) * array[i]\n",
    "        average_array[i] = V/(1 - pow(betta, i+1))\n",
    "\n",
    "    return average_array\n",
    "\n",
    "\n",
    "def mean_pooling(outputs, batch) -> torch.tensor:\n",
    "\n",
    "    attention_mask = batch['attention_mask']\n",
    "    embeddings = outputs['hidden_states'][-1]\n",
    "\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "\n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "\n",
    "    mean_pooled = summed/summed_mask\n",
    "\n",
    "    return mean_pooled\n",
    "\n",
    "def normilize(df_column: pd.DataFrame):\n",
    "\n",
    "    df_column = (df_column - df_column.mean())/df_column.std()\n",
    "\n",
    "    return df_column\n",
    "\n",
    "def get_cos_sim(df):\n",
    "    sentence_1 = df.text\n",
    "    sentence_2 = df.prompt_text\n",
    "\n",
    "    sentence_tokenizer = AutoTokenizer.from_pretrained('all-mpnet-base-v2')\n",
    "    sentence_model = AutoModel.from_pretrained('all-mpnet-base-v2')\n",
    "\n",
    "    cos_sim_list = []\n",
    "    sentence_model.to(device)\n",
    "\n",
    "    for i, summarie in enumerate(tqdm(sentence_1)):\n",
    "        prompt = sentence_2[i]\n",
    "        inputs = sentence_tokenizer([summarie, prompt], padding=True,\n",
    "                                    return_tensors='pt', truncation=True)\n",
    "        inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = sentence_model(**inputs)\n",
    "\n",
    "        sentence_embeddings = mean_pooling(outputs, inputs)\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "\n",
    "        score = nn.CosineSimilarity(dim=0)(sentence_embeddings[0], sentence_embeddings[1])\n",
    "\n",
    "        cos_sim_list.append(score.item())\n",
    "\n",
    "    return cos_sim_list\n",
    "\n",
    "def get_stat_features(df, text_col=\"text\"):\n",
    "\n",
    "    df[\"num_unique_words\"] = normilize(df[text_col].apply(lambda x: len(set(x.split()))))\n",
    "    df[\"num_words\"] = normilize(df[text_col].apply(lambda x: len(x.split())))\n",
    "    df[\"num_sentences\"] = normilize(df[text_col].apply(lambda x: len(x.split('.'))))\n",
    "\n",
    "    df[\"syntax_count\"] = normilize(df[text_col].apply(lambda x: x.count(\",\")\n",
    "                                                      + x.count(\"-\") + x.count(\";\") + x.count(\":\")))\n",
    "    df['smog_index'] = normilize(df[text_col].apply(lambda x: textstat.smog_index(x)))\n",
    "    df['cos_sim'] = normilize(pd.DataFrame(get_cos_sim(df)))\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SentenseData(Dataset):\n",
    "    \"\"\"\n",
    "    :params: path to csv file with summaries, path to csv file with prompts, 'score' param\\\n",
    "    defines which score is used: content/wording\n",
    "    \"\"\"\n",
    "    def __init__(self, summaries_path=summaries_train_path,\n",
    "                 prompts_path=prompts_train_path,\n",
    "                 score='wording', test=False, fold_test='ebad26'):\n",
    "        self.test = test\n",
    "        self.score_type = score\n",
    "        self.fold_test = fold_test\n",
    "\n",
    "        with open(prompts_path, encoding='utf-8') as f:\n",
    "            self.prompts = pd.read_csv(f)\n",
    "\n",
    "        with open(summaries_path, encoding='utf-8') as f:\n",
    "            summaries = pd.read_csv(f)\n",
    "            self.summaries = summaries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__summaries)\n",
    "\n",
    "    @property\n",
    "    def summaries(self):\n",
    "        return self.__summaries\n",
    "\n",
    "    @summaries.setter\n",
    "    def summaries(self, df):\n",
    "        if self.test:\n",
    "            self.__summaries = df[df.prompt_id == self.fold_test].reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            self.__summaries = df[df.prompt_id != self.fold_test].reset_index(drop=True)\n",
    "\n",
    "    @property\n",
    "    def prompts(self):\n",
    "        return self.__prompts\n",
    "\n",
    "    @prompts.setter\n",
    "    def prompts(self, file):\n",
    "        self.__prompts = file\n",
    "        # for i, text in enumerate(self.__prompts.prompt_text):\n",
    "        #     if len(text.split()) > 300:\n",
    "        #         self.__prompts.prompt_text[i] = short_text(text)\n",
    "\n",
    "    def get_batch_text(self, index):\n",
    "        summary_text = self.summaries.text[index]\n",
    "\n",
    "        if True:#self.score_type == 'wording':\n",
    "            res = tokenizer(summary_text, padding='max_length', return_tensors='pt', truncation=True)\n",
    "            res = {k:val.squeeze() for k, val in res.items()}\n",
    "            res['features'] = torch.tensor([self.summaries['num_unique_words'][index], self.summaries['num_words'][index],\n",
    "                                  self.summaries['num_sentences'][index],self.summaries['syntax_count'][index],\n",
    "                                  self.summaries['smog_index'][index], self.summaries['cos_sim'][index]], dtype=torch.float32)\n",
    "\n",
    "            return res\n",
    "\n",
    "        prompt_text = self.prompts.prompt_text[self.prompts.prompt_id ==\n",
    "                                               self.summaries.prompt_id[index]].item().replace('\\n','')\n",
    "\n",
    "        return tokenizer([summary_text, prompt_text], padding='max_length', return_tensors='pt', truncation=True)\n",
    "\n",
    "    def get_score(self, index):\n",
    "        if self.score_type == 'content':\n",
    "            score = self.summaries.content[index]\n",
    "\n",
    "        elif self.score_type == 'wording':\n",
    "            score = self.summaries.wording[index]\n",
    "\n",
    "        return torch.tensor(score)\n",
    "\n",
    "    def __getitem__(self, index) -> torch.tensor:\n",
    "\n",
    "        batch_text = self.get_batch_text(index)\n",
    "        batch_score = self.get_score(index)\n",
    "\n",
    "        return batch_text, batch_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class STSBertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Sentence Semantic Similarity Bert model\n",
    "    :param: seg_head=True for using segmentation head instead of cosine similarity\n",
    "    :param: freeze_weights=True to freeze BERT model's weights and train only the segmentation head\n",
    "    \"\"\"\n",
    "    def __init__(self, with_features=False, input_size=size):\n",
    "        super(STSBertModel, self).__init__()\n",
    "\n",
    "        self.word_embedding = deberta.DeBERTa(config=config_debert(), pre_trained=model_name)\n",
    "        self.cos_score = nn.CosineSimilarity(dim=0)\n",
    "        self.identity = nn.Identity()\n",
    "        self.input_size = input_size\n",
    "        self.with_features = with_features\n",
    "\n",
    "        if with_features:\n",
    "            self.n_features = 6\n",
    "            self.input_size += self.n_features\n",
    "\n",
    "        # for param in self.word_embedding.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.attention = SelfAttention(self.input_size)\n",
    "        self.block = AttentionBlock(self.input_size)\n",
    "\n",
    "        self.FC_head = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.input_size, self.input_size)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            # ('fc2', nn.Linear(1024, self.input_size)),\n",
    "            # ('relu2', nn.ReLU())\n",
    "            ]))\n",
    "\n",
    "        self.FC_output = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(self.input_size, 1)),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.word_embedding(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
    "        output = mean_pooling(output, x)\n",
    "\n",
    "        if self.with_features:\n",
    "            output = torch.cat((output, x['features']), dim=1)\n",
    "\n",
    "        output = output.unsqueeze(1)\n",
    "        output = self.attention(output)\n",
    "        output = self.FC_output(output)\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.fc = nn.Linear(input_dim, input_dim)\n",
    "        self.attention = SelfAttention(input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.attention(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x += identity\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def validate_model(model, loss_func):\n",
    "\n",
    "    losses_epoch_test = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            targets = targets.float().to(device)\n",
    "            if True:\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "            else:\n",
    "                outputs = torch.stack([model(sentence.to(device)) for sentence in inputs])\n",
    "\n",
    "\n",
    "            loss = torch.sqrt(loss_func(outputs.squeeze(), targets))\n",
    "\n",
    "\n",
    "            losses_epoch_test.append(loss.item())\n",
    "\n",
    "    return losses_epoch_test\n",
    "\n",
    "def train_model(n_epochs, lr, weight_decay, n_fold):\n",
    "\n",
    "    model = STSBertModel(with_features=True)\n",
    "    # model = ClassificationModel(n_features=7, n_fold=n_fold)\n",
    "\n",
    "    model.float()\n",
    "    model.to(device)\n",
    "\n",
    "    lr_head = 5e-04\n",
    "    params = [\n",
    "        {'params': model.word_embedding.parameters()},\n",
    "        {'params': model.attention.parameters(), 'lr': lr_head},\n",
    "        {'params': model.block.parameters(), 'lr': lr_head},\n",
    "        {'params': model.FC_head.parameters(), 'lr': lr_head},\n",
    "        {'params': model.FC_output.parameters(), 'lr': lr_head}\n",
    "    ]\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 2, gamma=0.5)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        losses_train_per_epoch = []\n",
    "\n",
    "        for j, batch in enumerate(tqdm(train_loader, position=0, leave=True), 1):\n",
    "            inputs, targets = batch\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                inputs = {k:val.squeeze().to(device) for k, val in inputs.items()}\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = torch.sqrt(loss_func(outputs.squeeze(), targets))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            scaler.update()\n",
    "\n",
    "            losses_train_per_epoch.append(loss.item())\n",
    "\n",
    "            if j%20 == 0 or j == len(train_loader):\n",
    "\n",
    "                losses_test_per_epoch = validate_model(model, loss_func)\n",
    "                losses_test_mean = np.mean(losses_test_per_epoch)\n",
    "                losses_train_mean = np.mean(losses_train_per_epoch)\n",
    "\n",
    "                losses_train.append(losses_train_mean)\n",
    "                losses_test.append(losses_test_mean)\n",
    "\n",
    "                if losses_test_mean == np.min(losses_test):\n",
    "                    best_score = losses_test_mean\n",
    "                    n_iter = len(losses_train)\n",
    "                    torch.save(model.state_dict(), r'models\\cv\\best__fold_{}.pt'.format(n_fold))\n",
    "                    print(f'Best saved, loss: {best_score:.5f}')\n",
    "\n",
    "                print(f'Fold# {n_fold}, Epoch: {i+1}/{n_epochs}, L_train: {losses_train_mean:.5f},\\\n",
    "                L_test: {losses_test_mean:.5f}')\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Time elapsed:{datetime.now()-t0}, Best score:{best_score:.5f}\")\n",
    "\n",
    "    return best_score, n_iter, losses_test, losses_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def loaders_init(score_type, batch_size, fold):\n",
    "\n",
    "    test_data = SentenseData(test=True, score=score_type, fold_test=fold)\n",
    "    train_data = SentenseData(score=score_type, fold_test=fold)\n",
    "\n",
    "    train_loader = DataLoader(train_data,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, drop_last=True)\n",
    "\n",
    "    test_loader = DataLoader(test_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def axis_set(ax):\n",
    "\n",
    "    ax.plot(test_losses, label='test')\n",
    "    ax.plot(train_losses, label='train')\n",
    "    ax.scatter(n_iter - 1, best_score, label='min')\n",
    "    ax.set_title(f'#{i}, min: {best_score:.5f}', loc='left')\n",
    "    ax.xaxis.set_major_locator(ticker.LinearLocator(n_epochs + 1))\n",
    "    ax.xaxis.set_major_formatter(ticker.FixedFormatter(range(n_epochs)))\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2023 15:38:44|INFO|logging|00| Loaded pretrained model file C:\\Users\\shmak\\.~DeBERTa/assets/latest/deberta-base\\pytorch_model.bin\n",
      "  4%|▍         | 20/510 [00:34<51:13,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 1.16936\n",
      "Fold# 1, Epoch: 1/3, L_train: 2.47591,                L_test: 1.16936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40/510 [01:08<49:14,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.71779\n",
      "Fold# 1, Epoch: 1/3, L_train: 1.79553,                L_test: 0.71779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/510 [01:41<47:03,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.53087\n",
      "Fold# 1, Epoch: 1/3, L_train: 1.43130,                L_test: 0.53087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 80/510 [02:14<43:58,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 1.27160,                L_test: 0.84677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 100/510 [02:47<42:06,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 1.13871,                L_test: 0.68065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 120/510 [03:20<39:46,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 1.05696,                L_test: 0.54395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 140/510 [03:54<38:42,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.50052\n",
      "Fold# 1, Epoch: 1/3, L_train: 0.98846,                L_test: 0.50052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 160/510 [04:27<35:27,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.93374,                L_test: 0.51274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 180/510 [04:59<33:09,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.90014,                L_test: 0.53866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 200/510 [05:32<31:15,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.87184,                L_test: 0.68851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 220/510 [06:05<29:50,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.49977\n",
      "Fold# 1, Epoch: 1/3, L_train: 0.84937,                L_test: 0.49977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 240/510 [06:38<27:14,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.82841,                L_test: 0.50676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 260/510 [07:11<25:39,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.81124,                L_test: 0.55497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 280/510 [07:44<23:15,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold# 1, Epoch: 1/3, L_train: 0.79324,                L_test: 0.50414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 300/510 [08:17<21:30,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best saved, loss: 0.48804\n",
      "Fold# 1, Epoch: 1/3, L_train: 0.77844,                L_test: 0.48804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 319/510 [08:31<02:23,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "# def cross_validation():\n",
    "with open(prompts_train_path, encoding='utf-8') as f:\n",
    "    prompts = pd.read_csv(f)\n",
    "#     prompts = prompts[prompts.prompt_id != '3b9047'].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "folds_list = list(prompts.prompt_id)\n",
    "score_type = 'content'\n",
    "batch_size = 10\n",
    "lr = 5e-04\n",
    "weight_decay = 1e-05\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "scores = []\n",
    "\n",
    "fig, axs = plt.subplots(1, len(folds_list), figsize=(12,4), layout='tight')\n",
    "\n",
    "for i, fold in enumerate(folds_list, 1):\n",
    "    train_loader, test_loader = loaders_init(score_type, batch_size, fold)\n",
    "\n",
    "    best_score, n_iter, test_losses, train_losses = train_model(n_epochs, lr, weight_decay, i)\n",
    "\n",
    "    scores.append(best_score)\n",
    "\n",
    "    axs[i-1] = axis_set(axs[i-1])\n",
    "\n",
    "mean_score = np.mean(scores)\n",
    "\n",
    "fig.suptitle(f'CV={mean_score:.5f}')\n",
    "fig.savefig('res_figure.png', dpi=200)\n",
    "\n",
    "mean_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
